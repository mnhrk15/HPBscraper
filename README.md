# ホットペッパービューティー スクレイピングツール

## 概要

このツールは、ホットペッパービューティーのウェブサイトから美容室の情報をスクレイピングするためのPythonアプリケーションです。エリアごとの美容室リスト、各美容室の詳細情報（名前、電話番号、住所、スタッフ数、関連リンクなど）を収集し、Excelファイルに出力する機能を提供します。

このツールは以下の2つの実行モードをサポートしています。

1.  **Streamlit Webアプリケーション**: `app.py` を使用して、Webブラウザから操作できるGUIを提供します。エリア選択、検索フィルター、スクレイピング開始・停止、進捗状況の確認などが可能です。

2.  **コマンドラインアプリケーション**: `main.py` を使用して、コマンドラインからスクレイピングを実行します。シンプルな操作でスクレイピングを行いたい場合に適しています。

## ファイル構成

```
├── area.csv             # スクレイピング対象エリアのリスト (CSVファイル)
├── app.py               # Streamlit Webアプリケーションのメインスクリプト
├── config.py            # 設定ファイル (リクエスト設定、リトライ設定、セレクタなど)
├── excel_exporter.py    # スクレイピングデータをExcelファイルに出力するモジュール
├── area_processor.py    # エリアデータ (area.csv) を処理し、サロン数を取得するスクリプト
├── http_client.py       # リトライ機能付きHTTPクライアントモジュール
├── logging_setup.py     # ロギング設定モジュール
├── main.py              # コマンドラインアプリケーションのメインスクリプト
├── parallel_scraper.py  # 並列スクレイピング処理モジュール
├── requirements.txt     # 必要なPythonライブラリのリスト
├── scraper.py           # 個別のWebページから情報をスクレイピングするモジュール
├── url_utils.py         # URL処理ユーティリティモジュール
└── README.md            # このREADMEファイル
```

## 実行に必要な環境

*   **Python**: 3.8以上
*   **pip**: Pythonのパッケージ管理システム

### 必要なPythonライブラリ

必要なライブラリは `requirements.txt` にリストされています。以下のコマンドでインストールできます。

```bash
pip install -r requirements.txt
```

### エリアリスト CSVファイル (area.csv) の準備

スクレイピング対象のエリアリストを `area.csv` ファイルに用意する必要があります。`area.csv` のフォーマットは以下の通りです。

```csv
prefecture,area,url,estimated_salons
東京都,新宿・高田馬場・代々木,https://beauty.hotpepper.jp/svcSA/macAA/salon/,
大阪府,梅田・京橋・福島・本町,https://beauty.hotpepper.jp/svcSB/macBA/salon/,
...
```

*   `prefecture`: 都道府県名
*   `area`: エリア名
*   `url`: ホットペッパービューティーのエリアページのURL
*   `estimated_salons`: (オプション) 推定サロン数。`area_processor.py` を実行すると自動的に入力されます。

`area.csv` はUTF-8エンコーディングで保存してください。

#### `area_processor.py` の実行 (サロン数自動取得)

`area.csv` にサロン数を自動的に入力したい場合は、`area_processor.py` を実行してください。

```bash
python area_processor.py
```

実行後、`area_structured.csv` が生成されます。このファイルがサロン数を含んだエリアリストになります。`area.csv` を `area_structured.csv` に置き換えてください。

**注意**: `area_processor.py` はウェブサイトにアクセスしてサロン数を取得するため、実行に時間がかかります。また、ウェブサイトの構造変更により正常に動作しなくなる可能性があります。  `area_processor.py` は以下の処理を行います。

1.  `area.csv` を読み込みます。
2.  各エリアのURLにアクセスし、HTMLを解析してサロン数を抽出します。
3.  サロン数を `estimated_salons` 列に書き込みます。
4.  結果を `area_structured.csv` に保存します。
5.  サロン数の統計情報（合計、平均、最大、サロン数0のエリア数）を表示します。

## 実行方法

### 1. Streamlit Webアプリケーションの実行 (`app.py`)

以下のコマンドでStreamlitアプリケーションを起動します。

```bash
streamlit run app.py
```

Webブラウザが自動的に開き、アプリケーションが起動します。

**Streamlitアプリケーションの使い方**:

1.  **サイドバー**:
    *   **検索フィルター**: エリア名で検索し、表示するエリアを絞り込むことができます。検索クエリを入力すると、リアルタイムで結果がフィルタリングされます。
    *   **エリア選択**: ドロップダウンメニューから都道府県とエリアを選択します。選択したエリアの統計情報（エリア数、総サロン数など）が表示されます。
    *   **統計情報**: 全体および検索結果の統計情報（都道府県数、エリア総数、サロン総数）が表示されます。サロン数トップ5エリアも確認できます。
2.  **メインエリア**:
    *   **使い方**: アプリケーションの基本的な使い方が説明されています。
    *   **ボタン**:
        *   **スクレイピング開始**: 選択したエリアのスクレイピングを開始します。処理中はボタンが無効になります。
        *   **処理を中断**: 実行中のスクレイピングを中断します。
    *   **進捗表示**:
        *   **プログレスバー**: スクレイピングの進捗状況を視覚的に表示します。
        *   **ステータスメッセージ**: 現在の処理状況やエラーメッセージを表示します。
        *   **メトリクス**: 処理済み件数、成功件数、エラー件数、進捗率、平均処理時間、残り時間などの詳細な進捗情報を表示します。
    *   **スクレイピング結果**: スクレイピングが完了すると、収集されたサロンデータが表形式で表示されます。サロン名、電話番号、住所、スタッフ数、サロンURLなどの情報が表示されます。また、総サロン数、平均スタッフ数、データ取得成功率などの基本統計情報も表示されます。
3.  **Excel出力**: スクレイピング完了後、結果がExcelファイル (`.xlsx`) として `output` ディレクトリに保存されます。ファイル名には、都道府県名、エリア名、実行日時が含まれます。

### 2. コマンドラインアプリケーションの実行 (`main.py`)

以下のコマンドでコマンドラインアプリケーションを実行します。

```bash
python main.py
```

**コマンドラインアプリケーションの使い方**:

1.  アプリケーション起動後、プロンプトでエリアページのURLを入力してください。
2.  スクレイピングが開始され、進捗状況がコンソールに表示されます。
3.  スクレイピング完了後、Excelファイルが生成され、保存パスがコンソールに表示されます。

## モジュールの詳細

*   **`area_processor.py`**:
    *   `AreaProcessor` クラス:
        *   `__init__`: セッションの初期化、ロギング設定。
        *   `setup_logging`: ロギング設定。ログファイルは `area_processing.log` に出力されます。
        *   `get_salon_count`: URLからサロン数を取得。複数のHTML構造に対応するためのパターンマッチングを使用。
        *   `process_areas`: エリアデータの処理メイン関数。`area.csv` を読み込み、各エリアのサロン数を取得して `area_structured.csv` に保存。統計情報を表示。
*   **`app.py`**:
    *   Streamlit Webアプリケーションのエントリーポイント。
    *   セッション状態の初期化、UIの構築、スクレイピング処理の制御を行う。
    *   進捗状況の表示、エラーハンドリング、Excel出力などの機能を提供する。
*   **`config.py`**:
    *   スクレイピングの設定や定数を管理。
    *   `HEADERS`: HTTPリクエストヘッダー。
    *   `MAX_RETRIES`, `RETRY_DELAY`, `RETRY_CODES`: リトライ設定。
    *   `REQUEST_TIMEOUT`, `SCRAPING_DELAY`: スクレイピング設定。
    *   `MAX_WORKERS`, `CHUNK_SIZE`, `RATE_LIMIT`: 並列処理設定。
    *   `LOG_FORMAT`, `LOG_FILE`, `LOG_LEVEL`: ロギング設定。
    *   `PHONE_SELECTORS`, `SALON_SELECTORS`: スクレイピング対象の要素を特定するためのCSSセレクタ。
*   **`excel_exporter.py`**:
    *   `ExcelExporter` クラス:
        *   `export_salon_data`: スクレイピングしたサロンデータをExcelファイルに出力。
*   **`http_client.py`**:
    *   `HTTPClient` クラス:
        *   `get`: リトライ機能付きのGETリクエストを実行。エクスポネンシャルバックオフとジッターを使用。
        *   `calculate_backoff`: エクスポネンシャルバックオフ時間を計算。
        *   `should_retry`: リトライすべきかどうかを判断。
*   **`logging_setup.py`**:
    *   `setup_logging`: アプリケーション全体のロギング設定を初期化。ファイルとコンソール出力を設定。ログファイルは `scraping.log` に出力されます。
*   **`main.py`**:
    *   コマンドラインアプリケーションのエントリーポイント。
    *   `main`: スクレイピングを実行し、Excelファイルを出力。
*   **`parallel_scraper.py`**:
    *   `ParallelScraper` クラス:
        *   `__init__`: 並列スクレイピングの設定を初期化。
        *   `scrape_salon_urls`: エリアページからサロンURLを収集。
        *   `scrape_salon_details_parallel`: サロン情報を並列で取得。
        *   `stop`: スクレイピング処理を中断。
        *   `reset`: 状態をリセット。
    *   `RateLimiter` クラス:
        *   `__init__`: レート制限の設定を初期化。
        *   `wait`: 必要な待機時間を計算して待機。
*   **`scraper.py`**:
    *   `BeautyScraper` クラス:
        *   `scrape_phone_number`: 電話番号ページから電話番号をスクレイピング。
        *   `scrape_salon_details`: サロン詳細ページから情報をスクレイピング。
        *   `scrape_salon_urls`: エリアページからサロンURLをスクレイピング。
*   **`url_utils.py`**:
    *   `normalize_url`: URLから不要なパラメータを削除し正規化。

## 設定ファイル (`config.py`) の調整

`config.py` ファイルには、スクレイピングの動作を制御するための重要な設定が含まれています。必要に応じて、以下の設定を調整してください。

*   **`HEADERS`**: HTTPリクエストヘッダー。User-Agentを変更することで、スクレイピングを検知されにくくすることができます。User-Agentリストを定義し、リクエストごとにランダムに選択するようにすると、さらに効果的です。
*   **`MAX_RETRIES`, `RETRY_DELAY`, `RETRY_CODES`**: リトライ設定。ウェブサイトへのアクセスが不安定な場合に、リトライ回数や待機時間を調整することで、スクレイピングの成功率を高めることができます。
*   **`REQUEST_TIMEOUT`**: リクエストタイムアウト。タイムアウト時間を調整することで、応答の遅いウェブサイトへの対応を改善できます。
*   **`SCRAPING_DELAY`**: スクレイピング遅延。ウェブサイトに負荷をかけないように、リクエスト間隔を調整します。
*   **`MAX_WORKERS`**: 並列処理のワーカー数。ワーカー数を増やすことで、スクレイピングの速度を向上させることができます。ただし、ワーカー数を増やしすぎると、ウェブサイトに負荷がかかり、アクセス制限を受ける可能性があります。
*   **`PHONE_SELECTORS`, `SALON_SELECTORS`**: スクレイピング対象の要素を特定するためのCSSセレクタ。ウェブサイトの構造が変更された場合は、これらのセレクタを修正する必要があります。

## 注意点

*   **ウェブサイトの利用規約の遵守**: スクレイピングを行う際は、ホットペッパービューティーの利用規約を遵守してください。robots.txt の内容も確認し、過度なアクセスや負荷をかける行為は避けてください。
*   **スクレイピング頻度**: 短時間に大量のアクセスを行うと、ウェブサイトからアクセス制限を受ける可能性があります。`config.py` の `SCRAPING_DELAY` や `RATE_LIMIT` を調整し、適切な間隔でアクセスするようにしてください。
*   **ウェブサイトの構造変更**: ホットペッパービューティーのウェブサイトの構造が変更された場合、スクレイピングツールが正常に動作しなくなる可能性があります。定期的にツールの動作確認を行い、必要に応じて修正してください。特に、`config.py` 内の `PHONE_SELECTORS` や `SALON_SELECTORS` は注意が必要です。
*   **エラーハンドリング**: エラーが発生した場合、ログファイル (`scraping.log`, `area_processing.log`) に詳細な情報が出力されます。エラーが発生した場合は、ログファイルを確認してください。
*   **個人情報の取り扱い**: スクレイピングによって個人情報 (電話番号など) が収集される場合があります。個人情報保護法などの関連法規を遵守し、適切な取り扱いを行ってください。
*   **CAPTCHA**: ウェブサイトがCAPTCHA (画像認証など) を導入した場合、自動スクレイピングが困難になる可能性があります。

## 既知の問題点

*   **ウェブサイトの構造変更への脆弱性**: ウェブサイトの構造が変更されると、セレクタ (`config.py` 内の `SALON_SELECTORS` など) の修正が必要になる場合があります。
*   **一部情報の取得失敗**: ウェブサイトの構造やデータ形式によっては、一部の情報 (電話番号、スタッフ数など) が取得できない場合があります。セレクタを調整することで改善される可能性があります。
*   **CAPTCHA**: ウェブサイトがCAPTCHA (画像認証など) を導入した場合、自動スクレイピングが困難になる可能性があります。CAPTCHA回避のための追加のライブラリやAPIの導入が必要になる場合があります。
*   **並列処理**: 並列処理中にエラーが発生した場合、処理が中断されることがあります。エラーハンドリングを強化することで、処理の継続性を高めることができます。
*   **進捗表示**: 進捗表示が正確でない場合があります。特に、残り時間の計算は推定値であり、実際の処理時間と異なる場合があります。

## 今後の改善点

*   **エラー処理の強化**: より詳細なエラーログの出力、エラー発生時の処理継続、リトライ処理の改善など。
*   **設定の柔軟性向上**: スクレイピング設定 (リクエストヘッダー、タイムアウト、リトライ回数など) を外部ファイルから設定できるようにする。
*   **User-Agentのランダム化**: `config.py` でUser-Agentリストを定義し、リクエストごとにランダムに選択するようにする。
*   **プロキシサポート**: プロキシサーバー経由でリクエストを送信する機能を追加し、アクセス制限を回避できるようにする。
*   **CAPTCHA回避**: CAPTCHAを自動で回避するためのライブラリやAPIを導入する。
*   **スクレイピング対象の拡張**: ホットペッパービューティーの他のページ (メニュー、口コミなど) もスクレイピングできるようにする。
*   **データ整形機能**: スクレイピングしたデータを整形するための機能を追加する。例えば、住所を都道府県、市区町村、番地などに分割する。
*   **データ保存先の拡張**: Excelファイル以外にも、CSVファイル、データベースなどに保存できるようにする。
*   **GUIの改善**: Streamlit WebアプリケーションのUIを改善し、より使いやすくする。例えば、スクレイピング設定をGUIから変更できるようにする。

## 免責事項

このツールは、学習・研究目的で作成されたものであり、商用利用や違法行為を助長するものではありません。ツールの利用によって生じたいかなる損害についても、作者は一切の責任を負いません。自己責任において、利用規約・法令を遵守し、適切な範囲でご利用ください。